{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the matplotlib graphics library and configure it to show \n",
    "# figures inline in the notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "In this article, we will focus on the selection of loss functions for regression and classification problems. First, we justify minimizing the mean square error (MSE) as a loss function for linear regression. We will proceed from the statement of the regression as a task with an infinite number of outcomes, thereby naturally assuming that the loss function, in this case, will be continuous. In contrast, the problem of classification has a discrete number of outcomes, and its loss function does not have the same nature as for regression. We will introduce a metric on the distribution space approximating the classification values and show that although the introduced value may not have all the properties of the metric, it can nevertheless serve to determine the “distance” between distributions, that is, it can be successfully used as a loss function. The considered quantity called cross-entropy, and it is widely used in commercial libraries (TensorFlow, PyTorch) when constructing classification models.\n",
    "\n",
    "## Maximum likelihood estimation (MLE) and KL Divergence\n",
    "There are many ways to introduce a metric on a distribution space. Some metrics were borrowed from functional analysis, while others, due to their special properties, were introduced for special cases. Such cases include pre-metrics that satisfy only part of the axiomatics of metrics, however, they are often used to specify the topology of the distribution space, and to some extent play the role of the distance on it. Such is the pre-metric that is known from information theory: the Kullback-Leibler divergence. For discrete distributions, it is defined as\n",
    "\n",
    "\n",
    "$$D_{KL}(P||Q)  =\\sum_{x \\in X} P(x) \\log(\\frac{P(x)}{Q(x)})$$\n",
    "\n",
    "And for continuous distributions:\n",
    "\n",
    "$$D_{KL}(P||Q)=\\int_{\\infty}^{-\\infty} p(x) \\log(\\frac{p(x)}{q(x)}) dx$$\n",
    "\n",
    "This divergence is not symmetrical and does not satisfy the triangle inequality:\n",
    "$$D_{KL}(P||Q) \\neq D_{KL} (Q||P)$$\n",
    "\n",
    "The only fact that the Kullback-Leibler divergence is related to the metric is that it is not negative and is equal to zero only for $P=Q$ almost everywhere.\n",
    "\n",
    "In order to explain the meaning of the introduced quantity, let us step back and try to formalize the intuitive idea that the amount of information that an event carries is the greater the less this event, i.e. the less likely the event, the more informative it is.\n",
    "\n",
    "This is expressed well by the function on the graph below:\n",
    "![](https://raw.githubusercontent.com/olegkleiman/ml_theory/master/images/MinusLog.png)\n",
    "\n",
    "The probability of the event is plotted on the  axis, its “amount of information\" on the axis. You can notice that this function on the segment $[0\\le x \\le 1]$ fits perfectly to the given intuitive expression. \n",
    "\n",
    "1. It takes 0 on a value of 1 - the maximum allowable probability value, i.e. the information contained in the event that is certain to happen (with probability 1) is zero.\n",
    "2. The lower the probability of an event, the greater its information: $\\lim_{x \\to +0}=\\infty$\n",
    "3. $ \\forall x \\in [0 \\ge x \\ge 1]: I(x) \\ge 0 $\n",
    "\n",
    "The considered value was introduced by C. Shannon in the epoch-making work [4], and received the name of the event’s self-information:\n",
    "$$I(x)=-log p(x)$$\n",
    "where the negative sign ensures that information is positive or zero.\n",
    "The choice of basis for the logarithm is arbitrary, but by the convention in information theory it is used to the base of 2.\n",
    "\n",
    "It is readily extended from a single event to the entire (discrete) distribution:\n",
    "$$H(X)=-\\sum_{i=1}^m p(x) \\times \\log p(x)$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('env': venv)",
   "language": "python",
   "name": "python37464bitenvvenv026775e8797f4f16bec2db2cf22d1b58"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}